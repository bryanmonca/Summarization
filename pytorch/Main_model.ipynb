{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1VZ6vYY9qVQ"
      },
      "source": [
        "# Encoder-Decoder with Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E1yzlyg945k"
      },
      "source": [
        "Pre-processing steps are based on: https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
        "\n",
        "Also the model used is based on the seq2seq translation tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "The idea was to try to handle summarization as a translation problem.\n",
        "\n",
        "Evaluation results and summaries were obtained using 10 observations. \n",
        "\n",
        "* BLEU-4 score for training: 0.53\n",
        "* BLEU-4 score for validation: 0.42"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxhRKaxRbG9r",
        "outputId": "66814927-0162-4a3b-aca7-e35c086b683b"
      },
      "source": [
        "# Importing libraries\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTPDpwe7iBQk",
        "outputId": "42711eba-b9aa-4f8c-f32c-6344c8f4fb9e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roP__B9O__zk"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTvYZU5VARXY"
      },
      "source": [
        "The data we are working with is amazon food reviews, it is publicly available in [kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews).\n",
        "\n",
        "In order to have the data ready for the model, we followed the following steps:\n",
        "* Drop duplicates and NA values\n",
        "* Expand Contractions (Ex: aren't -> are not)\n",
        "* Text to lowercase, remove html tags, remove ('s) \n",
        "* Remove text inside parenthesis, eliminate punctuations and special characters\n",
        "* Remove stopwords, and remove short words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMVGhfHde-ne"
      },
      "source": [
        "# Import Data\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Import data\n",
        "data = pd.read_csv(\"/content/drive/My Drive/reviews.csv\", nrows=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uAt71JIgl20"
      },
      "source": [
        "# Drop duplicates and NA values\n",
        "data.drop_duplicates(subset=['Text'], inplace=True)\n",
        "data.dropna(axis=0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgwnjxINf4cQ"
      },
      "source": [
        "# Expanded contractions\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI76tq7egGjL"
      },
      "source": [
        "# Text cleaning\n",
        "# lowercase, remove html tags, contraction mapping, remove ('s), remove text inside parenthesis,\n",
        "# eliminate punctuations and special characters, remove stopwords, remove short words.\n",
        "stop_words = set(stopwords.words('english')) \n",
        "def text_cleaner(text):\n",
        "    new_string = text.lower()\n",
        "    new_string = BeautifulSoup(new_string, \"lxml\").text\n",
        "    new_string = re.sub(r'\\([^)]*\\)', '', new_string)\n",
        "    new_string = re.sub('\"', '', new_string)\n",
        "    new_string = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_string.split(\" \")])    \n",
        "    new_string = re.sub(r\"'s\\b\", \"\", new_string)\n",
        "    new_string = re.sub(\"[^a-zA-Z]\", \" \", new_string) \n",
        "    tokens = [w for w in new_string.split() if not w in stop_words]\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>=3:                  #removing short word\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "cleaned_text = []\n",
        "for t in data['Text']:\n",
        "    cleaned_text.append(text_cleaner(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsFwdOI3jn2f"
      },
      "source": [
        "# Summary Cleaning\n",
        "# lowercase, remove html tags, contraction mapping, remove ('s), remove text inside parenthesis,\n",
        "# eliminate punctuations and special characters, remove stopwords, remove short words.def summary_cleaner(text):\n",
        "    new_string = re.sub('\"','', text)\n",
        "    new_string = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_string.split(\" \")])    \n",
        "    new_string = re.sub(r\"'s\\b\", \"\", new_string)\n",
        "    new_string = re.sub(\"[^a-zA-Z]\", \" \", new_string)\n",
        "    new_string = new_string.lower()\n",
        "    tokens=new_string.split()\n",
        "    new_string=''\n",
        "    for i in tokens:\n",
        "        if len(i)>1:                                 \n",
        "            new_string=new_string + i + ' '  \n",
        "    return new_string.strip()\n",
        "\n",
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(summary_cleaner(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H-Q5KjykUtC"
      },
      "source": [
        "# Cleaning text and summary columns\n",
        "data['cleaned_text'] = cleaned_text\n",
        "data['cleaned_summary'] = cleaned_summary\n",
        "data['cleaned_summary'].replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XJM8NO40yAc"
      },
      "source": [
        "# Assigning pandas series to variables\n",
        "text = data['cleaned_text']\n",
        "summary = data['cleaned_summary']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAMOS1CYCdUu"
      },
      "source": [
        "### Data Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrF9bU2_CicZ"
      },
      "source": [
        "Our data will be splitted using 90% train data and 10% validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6fyjigUypyj"
      },
      "source": [
        "## Data Splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(text, summary, test_size=0.1, \n",
        "                                                  random_state=0, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYmuptEVDDLd"
      },
      "source": [
        "We need to create a vocabulary for the text (review) and the summary. Here, they are treated as if each one of them were a Language. Both of them will have their own vocabulary and word to index dictionary. Also, we include a Start of Sentence (SOS), End of Sentence (EOS), and unknown (UNK) tokens in each language. The UNK token is necessary specially for our validation data because there are words that are not in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ0b2BaH6ITH"
      },
      "source": [
        "UNK_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "# Creates a vocabulary for a language. word to index, index to word,\n",
        "# and word frequency dictionaries are created.\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"<UNK>\":0}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"<UNK>\", 1: \"<SOS>\", 2: \"<EOS>\"}\n",
        "        self.n_words = 3  # Count SOS, EOS, and UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in word_tokenize(sentence):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xxkIho_7b4a"
      },
      "source": [
        "# Returns an instance of the Lang class for each of the text and summary\n",
        "# provided. Also, a list of text-summary pairs is returned.\n",
        "def readLangs(lang1, lang2):\n",
        "    pairs = list(zip(lang1, lang2))\n",
        "    input_lang = Lang('text')\n",
        "    output_lang = Lang('summary')\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnPvV1AcvuC5"
      },
      "source": [
        "# We filter each pair of text-summary to a certain length\n",
        "text_max_len = 80\n",
        "summary_max_len = 10\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < text_max_len and \\\n",
        "        len(p[1].split(' ')) < summary_max_len\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU5o-nui8TAS",
        "outputId": "f8689512-367f-4537-bfd4-298a961d21cf"
      },
      "source": [
        "# Prepares data by calling previous functions.\n",
        "# Reads data, filters it, and creates a vocabulary.\n",
        "def prepareData(lang1, lang2):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData(x_train, y_train)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 79516 sentence pairs\n",
            "Trimmed to 70168 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "text 40673\n",
            "summary 12573\n",
            "('wow unlike snack health bar ever cocoa mole great flavor nice chocolatey kick chili pepper works great mid morning snack like almost larabar flavors except banana kind bland hardly wait try key lime pie pistachio flavors wish bars sold regular grocery stores long drive wild oats live', 'only wish could buy these closer to home')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SziO7EMAc31b",
        "outputId": "67e0e36f-7de1-44b5-bac4-061717c12712"
      },
      "source": [
        "pairs[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('work bar cup tea delicious held lunch later day great snack eat breakfast toss purse mid day munchie',\n",
              " 'yummy breakfast snack')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpazfCzmg8sT",
        "outputId": "5e935a4d-01fa-4ef7-d2e6-1c145416aab9"
      },
      "source": [
        "val_pairs = list(zip(x_val, y_val))\n",
        "print(\"Read %s sentence pairs\" % len(val_pairs))\n",
        "val_pairs = filterPairs(val_pairs)\n",
        "print(\"Trimmed to %s sentence pairs\" % len(val_pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 8836 sentence pairs\n",
            "Trimmed to 7768 sentence pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-1-dYwthuxb",
        "outputId": "ab7e9522-e58b-4183-9320-6ac40afd41fa"
      },
      "source": [
        "val_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ordered salmon thursday january received january salmon delicious wooden box nice design used store items future',\n",
              " 'alaska smokehouse smoked salmon')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsJiioKTzAh7"
      },
      "source": [
        "# Vectorizing the data and converting each sentence into a tensor\n",
        "# To train for each pair, we need an input tensor and a target tensor\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index.get(word, lang.word2index['<UNK>']) for word in word_tokenize(sentence)]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGjdf-17E7xl"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIGhtW_P8icy"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UHE4ayWzUh1"
      },
      "source": [
        "## Decoder (Attention)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp8vkdAUXsPj"
      },
      "source": [
        "## Attention-Decoder\n",
        "\n",
        "# Because there are sentences of all sizes in the training data, to actually \n",
        "# create and train this layer we have to choose a maximum sentence length \n",
        "# (input length, for encoder outputs) that it can apply to. Sentences of the \n",
        "# maximum length will use all the attention weights, while shorter sentences \n",
        "# will only use the first few.\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=text_max_len):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)   #\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)    #\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "        \n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7LlwH9S9Dxv"
      },
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTiQs6D3QlNF"
      },
      "source": [
        "Training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha5tlkU-1G4m"
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
        "          decoder_optimizer, criterion, max_length=text_max_len):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    \n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    for di in range(target_length):\n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach() # detach from history as input\n",
        "        \n",
        "        loss += criterion(decoder_output, target_tensor[di])\n",
        "        if decoder_input.item() == EOS_token:\n",
        "            break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41fmWZCKCe87"
      },
      "source": [
        "# Estimate of time left in the training process. \n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsmsYvSUCog1"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100,\n",
        "               learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0    # Reset every print_every\n",
        "    plot_loss_total = 0     # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor[:text_max_len], target_tensor[:summary_max_len], \n",
        "                     encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
        "                     criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "            \n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxeS000hFTph"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMoLb9EbFaVi"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Cf73o9Qwps"
      },
      "source": [
        "The evaluation process is similar to the training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXWRIuaMFbvY"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=text_max_len):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(summary_max_len, max_length)\n",
        "\n",
        "        for di in range(summary_max_len):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KpmZo8KHZNn"
      },
      "source": [
        "# Generates a summary and compares it with the target summary in the data set\n",
        "def evaluateRandomly(encoder, decoder, pairs=pairs, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKHMLsZ2H56Q"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Calculates BLEU-1, BLEU-2, BLEU-3, BLEU-4 score for a number of a dataset \n",
        "# observations.\n",
        "def evaluate_bleu(encoder, decoder, pairs=pairs, n=10):\n",
        "    \"\"\"To speed up testing, we only evaluate BLEU score on n test sentences.\"\"\"\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for pair in pairs[:n]:\n",
        "        references.append([pair[1].split(' ')])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0][:text_max_len])\n",
        "        predictions.append(output_words)\n",
        "    blue_1 = corpus_bleu(references, predictions, weights=(1, 0, 0, 0))\n",
        "    blue_2 = corpus_bleu(references, predictions, weights=(0.5, 0.5, 0, 0))\n",
        "    blue_3 = corpus_bleu(references, predictions, weights=(0.33, 0.33, 0.33, 0))\n",
        "    blue_4 = corpus_bleu(references, predictions, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    print('BLEU-1 score:', blue_1)\n",
        "    print('BLEU-2 score:', blue_2)\n",
        "    print('BLEU-3 score:', blue_3)\n",
        "    print('BLEU-4 score:', blue_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT78Rid2IJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09cf123a-e3e7-4c01-de64-bf255a607596"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "# TO speed up training, you can reduce 75000 to 5000\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3m 54s (- 54m 39s) (5000 6%) 3.5162\n",
            "7m 18s (- 47m 32s) (10000 13%) 3.2460\n",
            "10m 44s (- 42m 58s) (15000 20%) 3.2176\n",
            "14m 9s (- 38m 56s) (20000 26%) 3.2461\n",
            "17m 34s (- 35m 9s) (25000 33%) 3.1720\n",
            "20m 59s (- 31m 29s) (30000 40%) 3.2241\n",
            "24m 26s (- 27m 56s) (35000 46%) 3.2794\n",
            "27m 52s (- 24m 23s) (40000 53%) 3.1959\n",
            "31m 19s (- 20m 52s) (45000 60%) 3.2170\n",
            "34m 47s (- 17m 23s) (50000 66%) 3.2252\n",
            "38m 13s (- 13m 54s) (55000 73%) 3.2018\n",
            "41m 40s (- 10m 25s) (60000 80%) 3.2094\n",
            "45m 9s (- 6m 56s) (65000 86%) 3.2086\n",
            "48m 37s (- 3m 28s) (70000 93%) 3.2004\n",
            "52m 6s (- 0m 0s) (75000 100%) 3.2178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C29Ip0fhRACl"
      },
      "source": [
        "### Training set evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSF8jC-CIQ8o",
        "outputId": "19ecdc6e-3066-4271-cc25-1c487c6c5212"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1) # n=10\n",
        "evaluate_bleu(encoder1, attn_decoder1) # n=10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> coffee snob perspective coffee really good great good drinking says lot really love supporting fair trade cause support get drink coffee help someone time good green mountain national wildlife blend cups also reviewed good would buy hope helps believe pod coffee measure\n",
            "= fair trade coffee trade off\n",
            "< great coffee <EOS>\n",
            "\n",
            "> lowest fat gms serving still tastes great flavor bit spicy much awesome manf plz continue make flavor thanks\n",
            "= one of the best\n",
            "< great <EOS>\n",
            "\n",
            "> gotten amazon add white corn syurp lbs corn sugar\n",
            "= great stuff\n",
            "< very <EOS>\n",
            "\n",
            "> tea good light flavor find drinking day feel healthier tired usually\n",
            "= white tea\n",
            "< best tea tea <EOS>\n",
            "\n",
            "> love buy another box done last one\n",
            "= yummy\n",
            "< best <EOS>\n",
            "\n",
            "> taste bad plus concentrate dont think work outrecovery effects dont get wrong zito great idea able take drink gym orbiking much prefer new vita coco new screw top much better flavorsone pretty good good vita coco one good thosecans terrible\n",
            "= this is not good\n",
            "< great <EOS>\n",
            "\n",
            "> took chance coffee becaused rating gave one star pleasantly surprised make coffee one cup time mesh camping filter drink immediately thus getting benefits flavor leaves atmosphere hopefully quality control cup also pleasant\n",
            "= very good cup of coffee\n",
            "< great coffee <EOS>\n",
            "\n",
            "> love bars really cannot believe gluten free truely would successful without bakery main products available gluten free items taste good gotten recipes right sign online great sending meal ideas coupons\n",
            "= winner\n",
            "< great <EOS>\n",
            "\n",
            "> fear super energy supplement drink crisp fruity taste like pomegranate ounce supposed servings tab pulled drink beverage lose fizz daily recommended vitamin vitamin vitamin price prohibitive think need particular product prime shipping pay close per probably higher retail paying shipping cost\n",
            "= fruity taste high priced\n",
            "< great good <EOS>\n",
            "\n",
            "> convenient great taste aroma crema fantastic product quick coffee morning leisurely occasion friends later day\n",
            "= dolce gusto lungo\n",
            "< great <EOS>\n",
            "\n",
            "BLEU-1 score: 0.1189807217549214\n",
            "BLEU-2 score: 0.32220131764942894\n",
            "BLEU-3 score: 0.4520938035463796\n",
            "BLEU-4 score: 0.5302158042203026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqAiL3T5iBNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b881c4-05a3-461f-9b19-c9e6770c3e03"
      },
      "source": [
        "evaluate_bleu(encoder1, attn_decoder1, n=len(pairs)) # whole train set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1 score: 0.05106646888486554\n",
            "BLEU-2 score: 0.014961365257071714\n",
            "BLEU-3 score: 0.05104827130597533\n",
            "BLEU-4 score: 0.09095135792801819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgH1z6gARKqM"
      },
      "source": [
        "### Validation set evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZZjbWfDduct",
        "outputId": "69f4382d-7737-4283-a063-17d7f031970e"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1, val_pairs) # n=10\n",
        "evaluate_bleu(encoder1, attn_decoder1, val_pairs) # n=10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> first time purchase brand san francisco bay coffee bought thinking original cups surprised opened box saw presentation thought bought kind tea cups mistake instead coffee cups coffee made another type coffee pots noooo ecologic new presentation love happy brand sure buying tasty coffee nice friendly presentation ecologic excellent gourmet taste happy love\n",
            "= very good and tasty gourmet coffee love presentation\n",
            "< great coffee <EOS>\n",
            "\n",
            "> bread comes alright given packaging contains packet yeast flavors added flour inclined believe really sourdough\n",
            "= not really sourdough is it\n",
            "< best <EOS>\n",
            "\n",
            "> love flavor beautiful corally colored sea salt another teriffic finishing salt adds color light sea flavor dish\n",
            "= alaea hawaiian sea salt\n",
            "< best <EOS>\n",
            "\n",
            "> guy pet store recommended young female cats would scarf barf grocery store dry food cats barf one morning unison enough bought days barf yet improvement anyone know exactly limited ingredient diet would help really able discern natural balance website thanks\n",
            "= no barf yet\n",
            "< great for <EOS>\n",
            "\n",
            "> ordered sinf basmati rice arrived today nissan extra long grain rice problems order arrived product pictured basmati rice extra long grain basmati paid pound bag sinf basmati rice sure happened feel like paid premium price rice even want beware get rice pictured pay premium price disappointment top even option return according amazon return policy even though sealed\n",
            "= wrong product misleading picture\n",
            "< not but <EOS>\n",
            "\n",
            "> mccormick sea salt delicious much better regular table salt coarse texture fine flavor help grind crystals salt bottle comes grinder top turn twist downside sometimes guests know\n",
            "= sea salt is delicious\n",
            "< delicious <EOS>\n",
            "\n",
            "> best chai tea ever like spices chai tea one extra spices\n",
            "= the best chai tea\n",
            "< best best <EOS>\n",
            "\n",
            "> get wrong cereal tastes delicious nice crunchy texture good nutty flavor sweet glaze buy cereal taste like gourmet honey nut cheerios delicious said vitamin enriched cereals similar amounts sugar trade sugar good crunchy nut grams sugar per cup serving malt meal honey nut scooters grams per cup serving totally comparable scooters calcium zinc twice iron folic acid also crunchy nut costs times much scooters advice taste king consideration buy cereal nutritious economical options\n",
            "= tastes great average nutrition very expensive\n",
            "< tasty <EOS>\n",
            "\n",
            "> using namaste bread mix couple years really like first went gluten free tried many different brands pre baked bread well bread mixes always sweet taste excited found namaste mix sweet live hour away closest store sells mix looked online found amazon buy carton saves time gas highly recommend anyone needs avoid gluten tasty husband even likes\n",
            "= best gluten free bread\n",
            "< great <EOS>\n",
            "\n",
            "> used really like stuff first came years ago see bacon mess good thing due religous reasons longer eat quick meal poor single girl way back love anything ranch mess\n",
            "= what the heck it is got bacon now\n",
            "< great <EOS>\n",
            "\n",
            "BLEU-1 score: 0.09760148895356403\n",
            "BLEU-2 score: 0.2582292673607482\n",
            "BLEU-3 score: 0.3594778453017897\n",
            "BLEU-4 score: 0.420029673833384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH6cC3SWiVw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "329170e1-1c95-4bb2-b17e-b4dc7ee34632"
      },
      "source": [
        "evaluate_bleu(encoder1, attn_decoder1, val_pairs, n=len(val_pairs)) # whole val set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1 score: 0.04917602156964814\n",
            "BLEU-2 score: 0.014146969161966937\n",
            "BLEU-3 score: 0.04936477077771504\n",
            "BLEU-4 score: 0.0888854425456694\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}