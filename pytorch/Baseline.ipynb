{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1VZ6vYY9qVQ"
      },
      "source": [
        "# Base line model - Encoder-Decoder without Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E1yzlyg945k"
      },
      "source": [
        "Pre-processing steps are based on: https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/\n",
        "\n",
        "Also the model used is based on the seq2seq translation tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "The idea was to try to handle summarization as a translation problem.\n",
        "\n",
        "Evaluation results and summaries were obtained using 10 observations. \n",
        "\n",
        "* BLEU-4 score for training: 0.57\n",
        "* BLEU-4 score for validation: 0.46"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxhRKaxRbG9r",
        "outputId": "d8bbb3fb-21da-4a07-c52c-b93bc353bd5b"
      },
      "source": [
        "# Importing libraries\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTPDpwe7iBQk",
        "outputId": "f40a33b6-a771-4418-94e8-bfd18ec2ed3a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roP__B9O__zk"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTvYZU5VARXY"
      },
      "source": [
        "The data we are working with is amazon food reviews, it is publicly available in [kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews).\n",
        "\n",
        "In order to have the data ready for the model, we followed the following steps:\n",
        "* Drop duplicates and NA values\n",
        "* Expand Contractions (Ex: aren't -> are not)\n",
        "* Text to lowercase, remove html tags, remove ('s) \n",
        "* Remove text inside parenthesis, eliminate punctuations and special characters\n",
        "* Remove stopwords, and remove short words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMVGhfHde-ne"
      },
      "source": [
        "# Import Data\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "data = pd.read_csv(\"/content/drive/My Drive/reviews.csv\", nrows=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uAt71JIgl20"
      },
      "source": [
        "# Drop duplicates and NA values\n",
        "data.drop_duplicates(subset=['Text'], inplace=True)\n",
        "data.dropna(axis=0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgwnjxINf4cQ"
      },
      "source": [
        "# Expanded contractions\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI76tq7egGjL"
      },
      "source": [
        "# Text cleaning\n",
        "# lowercase, remove html tags, contraction mapping, remove ('s), remove text inside parenthesis,\n",
        "# eliminate punctuations and special characters, remove stopwords, remove short words.\n",
        "stop_words = set(stopwords.words('english')) \n",
        "def text_cleaner(text):\n",
        "    new_string = text.lower()\n",
        "    new_string = BeautifulSoup(new_string, \"lxml\").text\n",
        "    new_string = re.sub(r'\\([^)]*\\)', '', new_string)\n",
        "    new_string = re.sub('\"', '', new_string)\n",
        "    new_string = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_string.split(\" \")])    \n",
        "    new_string = re.sub(r\"'s\\b\", \"\", new_string)\n",
        "    new_string = re.sub(\"[^a-zA-Z]\", \" \", new_string) \n",
        "    tokens = [w for w in new_string.split() if not w in stop_words]\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>=3:                  #removing short word\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()\n",
        "\n",
        "cleaned_text = []\n",
        "for t in data['Text']:\n",
        "    cleaned_text.append(text_cleaner(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsFwdOI3jn2f"
      },
      "source": [
        "# Summary Cleaning\n",
        "# lowercase, remove html tags, contraction mapping, remove ('s), remove text inside parenthesis,\n",
        "# eliminate punctuations and special characters, remove stopwords, remove short words.\n",
        "def summary_cleaner(text):\n",
        "    new_string = re.sub('\"','', text)\n",
        "    new_string = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in new_string.split(\" \")])    \n",
        "    new_string = re.sub(r\"'s\\b\", \"\", new_string)\n",
        "    new_string = re.sub(\"[^a-zA-Z]\", \" \", new_string)\n",
        "    new_string = new_string.lower()\n",
        "    tokens=new_string.split()\n",
        "    new_string=''\n",
        "    for i in tokens:\n",
        "        if len(i)>1:                                 \n",
        "            new_string=new_string + i + ' '  \n",
        "    return new_string.strip()\n",
        "\n",
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(summary_cleaner(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H-Q5KjykUtC"
      },
      "source": [
        "# Cleaning text and summary columns\n",
        "data['cleaned_text'] = cleaned_text\n",
        "data['cleaned_summary'] = cleaned_summary\n",
        "data['cleaned_summary'].replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XJM8NO40yAc"
      },
      "source": [
        "# Assigning pandas series to variables\n",
        "text = data['cleaned_text']\n",
        "summary = data['cleaned_summary']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAMOS1CYCdUu"
      },
      "source": [
        "### Data Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrF9bU2_CicZ"
      },
      "source": [
        "Our data will be splitted using 90% train data and 10% validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6fyjigUypyj"
      },
      "source": [
        "## Data Splitting\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(text, summary, test_size=0.1, \n",
        "                                                  random_state=0, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYmuptEVDDLd"
      },
      "source": [
        "We need to create a vocabulary for the text (review) and the summary. Here, they are treated as if each one of them were a Language. Both of them will have their own vocabulary and word to index dictionary. Also, we include a Start of Sentence (SOS), End of Sentence (EOS), and unknown (UNK) tokens in each language. The UNK token is necessary specially for our validation data because there are words that are not in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ0b2BaH6ITH"
      },
      "source": [
        "UNK_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "# Creates a vocabulary for a language. word to index, index to word,\n",
        "# and word frequency dictionaries are created.\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"<UNK>\":0}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"<UNK>\", 1: \"<SOS>\", 2: \"<EOS>\"}\n",
        "        self.n_words = 3  # Count SOS, EOS, and UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in word_tokenize(sentence):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xxkIho_7b4a"
      },
      "source": [
        "# Returns an instance of the Lang class for each of the text and summary\n",
        "# provided. Also, a list of text-summary pairs is returned.\n",
        "def readLangs(lang1, lang2):\n",
        "    pairs = list(zip(lang1, lang2))\n",
        "    input_lang = Lang('text')\n",
        "    output_lang = Lang('summary')\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnPvV1AcvuC5"
      },
      "source": [
        "# We filter each pair of text-summary to a certain length\n",
        "text_max_len = 80\n",
        "summary_max_len = 10\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < text_max_len and \\\n",
        "        len(p[1].split(' ')) < summary_max_len\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU5o-nui8TAS",
        "outputId": "7b89ed0d-82ff-4be2-eb2b-cf845f6be083"
      },
      "source": [
        "# Prepares data by calling previous functions.\n",
        "# Reads data, filters it, and creates a vocabulary.\n",
        "def prepareData(lang1, lang2):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData(x_train, y_train)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 79516 sentence pairs\n",
            "Trimmed to 70168 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "text 40673\n",
            "summary 12573\n",
            "('tarheel know ham great worth slice ham nice thick', 'close to home')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SziO7EMAc31b",
        "outputId": "d514bb4d-d0f2-4d59-b1b5-c0bc6a8df53c"
      },
      "source": [
        "pairs[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('work bar cup tea delicious held lunch later day great snack eat breakfast toss purse mid day munchie',\n",
              " 'yummy breakfast snack')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpazfCzmg8sT",
        "outputId": "24539ebf-c95d-4e27-fa76-9c200c2042aa"
      },
      "source": [
        "val_pairs = list(zip(x_val, y_val))\n",
        "print(\"Read %s sentence pairs\" % len(val_pairs))\n",
        "val_pairs = filterPairs(val_pairs)\n",
        "print(\"Trimmed to %s sentence pairs\" % len(val_pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 8836 sentence pairs\n",
            "Trimmed to 7768 sentence pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-1-dYwthuxb",
        "outputId": "cf3a9f6e-8ef3-4cb2-ca68-be26057eac94"
      },
      "source": [
        "val_pairs[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ordered salmon thursday january received january salmon delicious wooden box nice design used store items future',\n",
              " 'alaska smokehouse smoked salmon')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsJiioKTzAh7"
      },
      "source": [
        "# Vectorizing the data and converting each sentence into a tensor\n",
        "# To train for each pair, we need an input tensor and a target tensor\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index.get(word, lang.word2index['<UNK>']) for word in word_tokenize(sentence)]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGjdf-17E7xl"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIGhtW_P8icy"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mPAi4pwzPFF"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l06I5YTyHUit"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7LlwH9S9Dxv"
      },
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0d6LWRxM9gB"
      },
      "source": [
        "Training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha5tlkU-1G4m"
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, \n",
        "          decoder_optimizer, criterion, max_length=text_max_len):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "    \n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    for di in range(target_length):\n",
        "        decoder_output, decoder_hidden = decoder(\n",
        "            decoder_input, decoder_hidden)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach() # detach from history as input\n",
        "        \n",
        "        loss += criterion(decoder_output, target_tensor[di])\n",
        "        if decoder_input.item() == EOS_token:\n",
        "            break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41fmWZCKCe87"
      },
      "source": [
        "# Estimate of time left in the training process. \n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsmsYvSUCog1"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100,\n",
        "               learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0    # Reset every print_every\n",
        "    plot_loss_total = 0     # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor[:text_max_len], target_tensor[:summary_max_len], \n",
        "                     encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
        "                     criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "            \n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxeS000hFTph"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMoLb9EbFaVi"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9JQznfEMwnT"
      },
      "source": [
        "The evaluation process is similar to the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXWRIuaMFbvY"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=text_max_len):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(summary_max_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KpmZo8KHZNn"
      },
      "source": [
        "# Generates a summary and compares it with the target summary in the data set\n",
        "def evaluateRandomly(encoder, decoder, pairs=pairs, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKHMLsZ2H56Q"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Calculates BLEU-1, BLEU-2, BLEU-3, BLEU-4 score for a number of a dataset \n",
        "# observations.\n",
        "def evaluate_bleu(encoder, decoder, pairs=pairs, n=10):\n",
        "    \"\"\"To speed up testing, we only evaluate BLEU score on n test sentences.\"\"\"\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for pair in pairs[:n]:\n",
        "        references.append([pair[1].split(' ')])\n",
        "        output_words = evaluate(encoder, decoder, pair[0][:text_max_len])\n",
        "        predictions.append(output_words)\n",
        "    blue_1 = corpus_bleu(references, predictions, weights=(1, 0, 0, 0))\n",
        "    blue_2 = corpus_bleu(references, predictions, weights=(0.5, 0.5, 0, 0))\n",
        "    blue_3 = corpus_bleu(references, predictions, weights=(0.33, 0.33, 0.33, 0))\n",
        "    blue_4 = corpus_bleu(references, predictions, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    print('BLEU-1 score:', blue_1)\n",
        "    print('BLEU-2 score:', blue_2)\n",
        "    print('BLEU-3 score:', blue_3)\n",
        "    print('BLEU-4 score:', blue_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT78Rid2IJin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236c17d1-9de0-46a3-c19f-46784a2cfe88"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "# TO speed up training, you can reduce 75000 to 5000\n",
        "trainIters(encoder=encoder1, decoder=decoder1, n_iters=75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3m 23s (- 47m 33s) (5000 6%) 3.4754\n",
            "6m 20s (- 41m 13s) (10000 13%) 3.3138\n",
            "9m 16s (- 37m 5s) (15000 20%) 3.2615\n",
            "12m 13s (- 33m 37s) (20000 26%) 3.2162\n",
            "15m 10s (- 30m 21s) (25000 33%) 3.1839\n",
            "18m 9s (- 27m 14s) (30000 40%) 3.2550\n",
            "21m 7s (- 24m 8s) (35000 46%) 3.2479\n",
            "24m 6s (- 21m 5s) (40000 53%) 3.2676\n",
            "27m 7s (- 18m 4s) (45000 60%) 3.2018\n",
            "30m 4s (- 15m 2s) (50000 66%) 3.2109\n",
            "33m 2s (- 12m 0s) (55000 73%) 3.2610\n",
            "36m 1s (- 9m 0s) (60000 80%) 3.2110\n",
            "38m 59s (- 5m 59s) (65000 86%) 3.1853\n",
            "41m 58s (- 2m 59s) (70000 93%) 3.2226\n",
            "44m 55s (- 0m 0s) (75000 100%) 3.1843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvr5l3xMOcUO"
      },
      "source": [
        "### Training set evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSF8jC-CIQ8o",
        "outputId": "905faca4-e239-4eb5-8eda-beef6f0bc7d4"
      },
      "source": [
        "evaluateRandomly(encoder1, decoder1) # n=10\n",
        "evaluate_bleu(encoder1, decoder1) # n=10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> big fan primal strips fact loved first flavor much decided buy others really enjoyed exception mesquite lime luckily last flavor purchased displeasantly surprised rough time even swallowing first bite yes bad lime horridly instense vinegar like really taste like biting meaty textured product overbearing flavor vinegar may probably enjoy flavor otherwise like stick enjoyable tasty flavors wonderful vegan product offer\n",
            "= really terrible try another flavor don waste your money\n",
            "< not <EOS>\n",
            "\n",
            "> used taste thai peanut sauce mix years mixing packet one coconut milk makes easy use taste pretty good bit spice prepackaged peanut sauce like one best tried lots great pour vegetables meat rice even kids like\n",
            "= quick and easy\n",
            "< love <EOS>\n",
            "\n",
            "> savu smoker bags best grilling meat especially pork chops fish overwhelmed hickory taste especially salmon bags intended make grilling process extremely simple\n",
            "= mixed results\n",
            "< good <EOS>\n",
            "\n",
            "> year old border collie australian cattle dog mix food motivated bought toy reading positive reviews dog owners site lot dogs referenced labs retrievers assumed would perfect pooch figured rather quickly even though still loves play done finding treats less mins right throw toy around get treats continue use toy mostly work patience far challenging toy prove effective\n",
            "= good toy but not as challenging as wanted\n",
            "< my dog loves <EOS>\n",
            "\n",
            "> high quality dog food feed lamb easier disgestion highly recommend small dogs\n",
            "= high quality\n",
            "< my dog for <EOS>\n",
            "\n",
            "> blended week chianti rosso wine kits blended wine best wines produced wine full body smooth finish six weeks tast wine become complexed rich flavor also added cup corn sugar increase ach vol wine produced bottles\n",
            "= try this blend rosso chianti\n",
            "< excellent <EOS>\n",
            "\n",
            "> good value flavored coffee pound less price full pound lightly flavored medium roasted coffee using single cup brewer yeilds smooth non bitter light cup coffee havn tried using cup drip machine coffees little stronger done way\n",
            "= smooth light roast coffee good price to value\n",
            "< good coffee <EOS>\n",
            "\n",
            "> great bread tried quite brands frozen packaged think far one closest tasting bread regular bread please long holds also keeps well fridge like gluten free bread tried dry wrap air tight bag store shelf life unopened bread good time purchase case certainly worth money well spent bread also schar rolls noodles great luck crackers\n",
            "= very good bread\n",
            "< great <EOS>\n",
            "\n",
            "> crew loves tuna flakes fast know house stay side thinking get treat want treat kitty please try warning fast least belly rubs know money\n",
            "= good cat treat\n",
            "< what dog <EOS>\n",
            "\n",
            "> dog cookies good sadly box boxes arrived damaged box totally crushed great made good chirstmas gifts many dog friends\n",
            "= big sam review\n",
            "< my dog loves <EOS>\n",
            "\n",
            "BLEU-1 score: 0.1198986821386423\n",
            "BLEU-2 score: 0.3391246847822574\n",
            "BLEU-3 score: 0.48293057369596887\n",
            "BLEU-4 score: 0.5703374635148922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqAiL3T5iBNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd731f79-783a-4bf4-e226-edefcecfae03"
      },
      "source": [
        "evaluate_bleu(encoder1, decoder1, n=len(pairs)) # whole train set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1 score: 0.058464604539660014\n",
            "BLEU-2 score: 0.018125862109636334\n",
            "BLEU-3 score: 0.005942853806711276\n",
            "BLEU-4 score: 0.01819145913484394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ7ukpQSOXTX"
      },
      "source": [
        "### Validation set evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZZjbWfDduct",
        "outputId": "d6454f10-a78d-4a55-f00a-1d8246128d96"
      },
      "source": [
        "evaluateRandomly(encoder1, decoder1, val_pairs) # n=10\n",
        "evaluate_bleu(encoder1, decoder1, val_pairs) # n=10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> look futher always fresh nothing worse stale candy save anguish purchase today\n",
            "= fresh product\n",
            "< love <EOS>\n",
            "\n",
            "> really loves treats treats flips backwards get lol little bigger pencil eraser great training like cut half yield even tried cutting quarters seem little small dogs big mouth come resealable package helps keep nice fresh especially soft forgot close bag one day treats got pretty hard dog mind\n",
            "= his second favorite flavor\n",
            "< my dog loves for <EOS>\n",
            "\n",
            "> addicted potato chips love snacks may want grab picked first bag store miles home low fat flavor excellent crisp fresh packed looks like foil package grams fat per ounce best flavored chips found buying amazon brings chips door less cost buying local store need spend time money transportation get well packaged box bag sealed volume air keeps squashing breaking contents transit great flavorful low fat snack hits spot negative bags cabinet need ration pig enjoy\n",
            "= healthy chips at super price\n",
            "< yum <EOS>\n",
            "\n",
            "> last really delicious bread mix tastes like real bread used coconut oil instead vegetable oil added tablespoons ground flaxseed bread machine bit tricky mix old hand mixer rose beautifully baked perfectly smell freshly baked bread wonderful could wait cut loaf completely cool problem loaf moist tasted like real bread maybe flaxseed coconut oil helped bread fresh days right last crumb gobbled sliced thinly makes great toast fan bean flours bread much better others tried\n",
            "= pamela bread mix\n",
            "< delicious <EOS>\n",
            "\n",
            "> marcona almonds serious gourmet hungry shipped pound box good candy making company regular people love fresh nuts get fresh pecans sunnyland farms dot com fresh english walnuts almonds california almonds dot com prices shipping reasonable bigger orders discounted plus get email list notices new crops year used two sources excellent always shipping finest new crop nuts\n",
            "= excellent gourmet marcona almonds in huge box\n",
            "< perfect <EOS>\n",
            "\n",
            "> syrup good usually like maple syrup one delicious overly strong maple flavor sweet puchase based flavor organic great price made usa vermont maple syrup get better need use stonger shipping boxes wipe jugs sugar ants box syrup\n",
            "= highland sugarworks syrup is the best\n",
            "< good <EOS>\n",
            "\n",
            "> warning order contains ton great tasting gums come great flavors like gums certainly appreciate like firm still highly recommend like gummy candy\n",
            "= loved them\n",
            "< great <EOS>\n",
            "\n",
            "> normally care artificially sweetened drinks kind especially odd flavors like orange product actually quite tasty effects vary find bottle enough full bottle causes get jittery together though worth price\n",
            "= suprisingly tasty\n",
            "< good but <EOS>\n",
            "\n",
            "> brand right amount zip without harsh like much much better goya price right prime came free shipping happy purchase\n",
            "= just right\n",
            "< great <EOS>\n",
            "\n",
            "> local grocery store stopped stocking crown prince canned clams forced buy online instead opened cans side side snow shocked crown prince cans contain twice much clam meat drained snow addition crown prince clams pink look cleaner snow actually opened drained weighed number cans variety sure fluke\n",
            "= best canned clams have ever used\n",
            "< good <EOS>\n",
            "\n",
            "BLEU-1 score: 0.10048453012071468\n",
            "BLEU-2 score: 0.27822906525161284\n",
            "BLEU-3 score: 0.3933558519088639\n",
            "BLEU-4 score: 0.4629713772354549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH6cC3SWiVw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50abbd74-2474-4c2d-bd61-b07e1f1115a0"
      },
      "source": [
        "evaluate_bleu(encoder1, decoder1, val_pairs, n=len(val_pairs)) # whole val set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1 score: 0.05573315391126058\n",
            "BLEU-2 score: 0.017201009036260657\n",
            "BLEU-3 score: 0.005273955810186943\n",
            "BLEU-4 score: 0.01666750907661708\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}